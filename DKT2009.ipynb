{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DKT2009.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE4gegNGRCXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "class AssismentData():\n",
        "    def __init__(self):\n",
        "        self.data = pd.read_csv(\"/content/drive/My Drive/DKT/skill_builder_data.csv\")\n",
        "        self.data = self.data.dropna()\n",
        "        self.data[\"user_id\"], _ = pd.factorize(self.data[\"user_id\"])\n",
        "        self.data[\"skill_id\"], _ = pd.factorize(self.data[\"skill_id\"])\n",
        "        self.data[\"skills_correctness\"] = self.data.apply(\n",
        "            lambda x: x.skill_id * 2 if x.correct == 0.0 else x.skill_id * 2 + 1, axis=1)\n",
        "        self.data = self.data.groupby(\"user_id\").filter(lambda q: len(q) > 1).copy()\n",
        "        self.seq = self.data.groupby('user_id').apply(\n",
        "            lambda r: (\n",
        "                r[\"skills_correctness\"].values[:-1],\n",
        "                r[\"skill_id\"].values[1:],\n",
        "                r['correct'].values[1:]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def datasetReturn(self, shuffle=None, batch_size=32, val_data=None):\n",
        "\n",
        "        dataset = tf.data.Dataset.from_generator(lambda: self.seq, output_types=(tf.int32, tf.int32, tf.int32))\n",
        "\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=shuffle)\n",
        "\n",
        "        MASK_VALUE = -1\n",
        "        dataset = dataset.padded_batch(\n",
        "            batch_size=50,\n",
        "            padding_values=(MASK_VALUE,MASK_VALUE, MASK_VALUE),\n",
        "            padded_shapes=([None], [None], [None]),\n",
        "            drop_remainder=True\n",
        "        )\n",
        "        i = 0\n",
        "        for l in dataset.as_numpy_iterator():\n",
        "            i += 1\n",
        " \n",
        "        dataset = dataset.shuffle(buffer_size=50)\n",
        "        test_size = int(np.ceil(i * 0.2))\n",
        "        train_size = i - test_size\n",
        "      \n",
        "        train_data = dataset.take(train_size)\n",
        "        dataset = dataset.skip(train_size)\n",
        "\n",
        "        return train_data, dataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-TNIbrsSYQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ass = AssismentData()\n",
        "train_data,test_data = ass.datasetReturn()\n",
        "val_log = 'log/val'\n",
        "train_loss_log = 'log/train'\n",
        "summary_writer = tf.summary.create_file_writer(val_log)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFF2V5ZnDWvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "6bf72b5a-cdae-4110-9b37-26807195c696"
      },
      "source": [
        "print(ass.seq.head(5))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user_id\n",
            "0    ([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,...\n",
            "1    ([2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 5, 5, 5, 6, 8,...\n",
            "2    ([6, 14, 7, 9, 6, 8, 7, 9, 7, 9, 7, 17, 7, 17,...\n",
            "4    ([37, 34, 35, 39, 39, 49, 45, 43, 45, 45, 37, ...\n",
            "5    ([37, 41, 35, 39, 37, 37, 37, 35, 37, 35, 43, ...\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEDF0CHwScHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DKT(tf.keras.models.Model):\n",
        "    def __init__(self, total_skills_correctness, embedding_size):\n",
        "        super(DKT, self).__init__(name=\"DKTModel\")\n",
        "\n",
        "        self.mask = tf.keras.layers.Masking(mask_value=-1)\n",
        "\n",
        "        # 两个嵌入层\n",
        "       \n",
        "        self.skill_embedding = tf.keras.layers.Embedding(total_skills_correctness, embedding_size)\n",
        "        # RNN\n",
        "        self.rnn = tf.keras.Sequential ( [tf.keras.layers.LSTM(units=64, return_sequences=True, dropout=0.3),\n",
        "                            tf.keras.layers.LSTM(units=64, return_sequences=True, dropout=0.3)])\n",
        "\n",
        "        # dense\n",
        "        self.dense = tf.keras.layers.Dense(total_skills_correctness/2, activation='sigmoid')\n",
        "\n",
        "        self.distribute = tf.keras.layers.TimeDistributed(self.dense)\n",
        "\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "    def call(self, skillid):\n",
        "     \n",
        "        skillid = tf.expand_dims(skillid, axis=-1)\n",
        "\n",
        "        skillid = self.mask(skillid)\n",
        "\n",
        "\n",
        "        skill_vector = self.skill_embedding(skillid)\n",
        "\n",
        "        x = skill_vector\n",
        "\n",
        "        x = tf.squeeze(x, axis=-2)\n",
        "        x = self.rnn(x)\n",
        "        y = self.distribute(x)\n",
        "\n",
        "        return y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHzTdo7NSrfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "dbc7222c-9118-4853-fa5d-2dc1ae75f93c"
      },
      "source": [
        "dkt = DKT(int(ass.data[\"skills_correctness\"].max() + 1), 32)\n",
        "skill_num = int((ass.data[\"skills_correctness\"].max() + 1)/2)\n",
        "print(skill_num)\n",
        "AUC = tf.keras.metrics.AUC()\n",
        "VAUC = tf.keras.metrics.AUC()\n",
        "SCC = tf.keras.metrics.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "def test_one_step(sequence_id,skillid, label):\n",
        "    loss = dkt(skillid)\n",
        "\n",
        "    label = tf.expand_dims(label, axis=-1)\n",
        "\n",
        "    mask = 1. - tf.cast(tf.equal(label, -1),tf.float32)\n",
        "    \n",
        "    mask = tf.squeeze(mask)\n",
        "\n",
        "    squenceid = tf.boolean_mask(sequence_id, mask=mask)\n",
        "    squenceid = tf.one_hot(squenceid,depth=skill_num,axis=-1)\n",
        "    label = tf.boolean_mask(label, mask=mask)\n",
        "    loss = tf.boolean_mask(loss, mask=mask)\n",
        "\n",
        "    loss = tf.expand_dims(tf.reduce_sum(tf.multiply(squenceid , loss),axis=-1),axis=-1)\n",
        "\n",
        "    loss = tf.expand_dims(loss,axis=-1)\n",
        "\n",
        "    label = tf.squeeze(tf.one_hot(label,depth=2),axis=1)\n",
        "\n",
        "    loss = tf.concat([1-loss,loss],axis=1)\n",
        "    \n",
        "    VAUC.update_state(label , tf.squeeze(loss,axis=-1))\n",
        "\n",
        "\n",
        "def train_one_step(sequence_id,skillid, label):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = dkt(skillid)\n",
        "        \n",
        "        label = tf.expand_dims(label, axis=-1)\n",
        "  \n",
        "        mask = 1. - tf.cast(tf.equal(label, -1),tf.float32)\n",
        "        mask = tf.squeeze(mask)\n",
        "\n",
        "        sequenceid = tf.boolean_mask(sequence_id, mask=mask)\n",
        "        sequenceid = tf.one_hot(sequenceid,depth=skill_num,axis=-1)\n",
        "        label = tf.boolean_mask(label, mask=mask)\n",
        "        loss = tf.boolean_mask(loss, mask=mask)\n",
        "\n",
        "     \n",
        "        loss = tf.expand_dims(tf.reduce_sum(tf.multiply(sequenceid,loss),axis=-1),axis=-1)\n",
        "    \n",
        "        loss_real = tf.reduce_sum(tf.keras.losses.binary_crossentropy(label , loss))\n",
        "        \n",
        "        SCC.update_state(label, loss)\n",
        "  \n",
        "        loss = tf.expand_dims(loss,axis=-1)\n",
        "        \n",
        "        loss = tf.concat([1-loss,loss],axis=1)\n",
        "        \n",
        "        label = tf.squeeze(tf.one_hot(label,depth=2),axis=1)\n",
        "  \n",
        "        AUC.update_state(label , tf.squeeze(loss,axis=-1))\n",
        "        \n",
        "        gradients = tape.gradient(loss_real, dkt.trainable_variables)\n",
        "        # 反向传播，自动微分计算\n",
        "        optimizer.apply_gradients(zip(gradients, dkt.trainable_variables))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2uw9xkSTDL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "28e75831-4bb3-4b95-f2d6-8b68e81d77d8"
      },
      "source": [
        "for epoch in range(8):\n",
        "  train_data = train_data.shuffle(50)\n",
        "  AUC.reset_states()\n",
        "  VAUC.reset_states()\n",
        "  SCC.reset_states()\n",
        "  for s,p,l in train_data.as_numpy_iterator():\n",
        "    train_one_step(p,s,l)\n",
        "\n",
        "  for s,p,l in test_data.as_numpy_iterator():\n",
        "    test_one_step(p,s,l)\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('train_auc',AUC.result(),step=epoch)\n",
        "    tf.summary.scalar('val_auc',VAUC.result(),step=epoch)\n",
        "    \n",
        "  print(SCC.result(),AUC.result(),VAUC.result())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.64549065, shape=(), dtype=float32) tf.Tensor(0.6776958, shape=(), dtype=float32) tf.Tensor(0.7657523, shape=(), dtype=float32)\n",
            "tf.Tensor(0.56331336, shape=(), dtype=float32) tf.Tensor(0.7785327, shape=(), dtype=float32) tf.Tensor(0.8198595, shape=(), dtype=float32)\n",
            "tf.Tensor(0.49902382, shape=(), dtype=float32) tf.Tensor(0.83314186, shape=(), dtype=float32) tf.Tensor(0.8292503, shape=(), dtype=float32)\n",
            "tf.Tensor(0.48466077, shape=(), dtype=float32) tf.Tensor(0.84098864, shape=(), dtype=float32) tf.Tensor(0.8500039, shape=(), dtype=float32)\n",
            "tf.Tensor(0.47161525, shape=(), dtype=float32) tf.Tensor(0.8497119, shape=(), dtype=float32) tf.Tensor(0.8409797, shape=(), dtype=float32)\n",
            "tf.Tensor(0.46274725, shape=(), dtype=float32) tf.Tensor(0.8558189, shape=(), dtype=float32) tf.Tensor(0.8516734, shape=(), dtype=float32)\n",
            "tf.Tensor(0.46378323, shape=(), dtype=float32) tf.Tensor(0.85513514, shape=(), dtype=float32) tf.Tensor(0.8352248, shape=(), dtype=float32)\n",
            "tf.Tensor(0.46658456, shape=(), dtype=float32) tf.Tensor(0.85313046, shape=(), dtype=float32) tf.Tensor(0.8294535, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}